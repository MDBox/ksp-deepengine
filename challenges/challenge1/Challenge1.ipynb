{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "Fly as high as you can!\n",
    "\n",
    "## Objective\n",
    "Build a network that can launch a satallite rocket and fly it to the highest altitude possible. You can use the notebook below to get started. See if you can modify the training to include staging the rocket so it will go even higher.\n",
    "\n",
    "## Rules\n",
    "* You must use the provided ship\n",
    "* Don't escape Kerbin orbit\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libs\n",
    "import tensorflow as tf      \n",
    "import numpy as np\n",
    "import random                \n",
    "from random import randrange\n",
    "import time                  \n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import KSP Client Engine\n",
    "import sys\n",
    "from os.path import dirname\n",
    "sys.path.append(dirname('../../lib'))\n",
    "sys.path.append(dirname('../../agents'))\n",
    "from lib.DeepEngine.KSPDeepEngine import *\n",
    "from agents.BasicDQN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These function will help us parse and process the game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTFILTER = {}\n",
    "INPUTFILTER['currentStage'] = 'int'\n",
    "INPUTFILTER['velocityD'] = 'vector'\n",
    "INPUTFILTER['upAxis'] = 'vector'\n",
    "INPUTFILTER['up'] = 'vector'\n",
    "INPUTFILTER['north'] = 'vector'\n",
    "INPUTFILTER['east'] = 'vector'\n",
    "\n",
    "class Challenge1:    \n",
    "    def __init__(self):\n",
    "        # These inital values are set to 89M because that is the start alt of the episode\n",
    "        self.lastCheckAlt = 89\n",
    "        self.lastAltitude = 89\n",
    "        self.game = KSPDeepEngine()\n",
    "        \n",
    "    def parseVector3(self, vector3):\n",
    "        mag = math.sqrt(vector3['x'] * vector3['x'] + vector3['y'] * vector3['y'] + vector3['z'] * vector3['z'])\n",
    "        x = 0 \n",
    "        y = 0\n",
    "        z = 0\n",
    "\n",
    "        if mag != 0:\n",
    "            x = vector3['x']/mag\n",
    "            y = vector3['y']/mag\n",
    "            z = vector3['z']/mag\n",
    "        return [x,y,z]\n",
    "\n",
    "    def parseVessel(self, vessel):\n",
    "        values = []\n",
    "        for k in vessel:\n",
    "            if k in INPUTFILTER:\n",
    "                if INPUTFILTER[k] == 'int':\n",
    "                    values.append(vessel[k])\n",
    "                if INPUTFILTER[k] == 'float':\n",
    "                    values.append(math.atan(vessel[k]))\n",
    "                if INPUTFILTER[k] == 'vector':\n",
    "                    values.extend(self.parseVector3(vessel[k]))            \n",
    "        return values\n",
    "\n",
    "    def parseState(self, action, state, game):\n",
    "        done = False\n",
    "        vessel = action.vessel\n",
    "        state.extend(self.parseVessel(vessel))    \n",
    "        reward = 0\n",
    "        \n",
    "        if vessel['altitude'] - self.lastCheckAlt > 10:\n",
    "            reward = 1\n",
    "            self.lastCheckAlt = vessel['altitude']\n",
    "        elif self.lastCheckAlt - vessel['altitude'] > 10:\n",
    "            reward = -1\n",
    "            self.lastCheckAlt = vessel['altitude']\n",
    "\n",
    "        # Episode is over if action is = 3 or vessel drops below 75 M\n",
    "        if action.action == 3 or int(vessel['altitude']) <= 75: # Action 3 means ship crashed\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        if int(vessel['altitude']) != self.lastAltitude:\n",
    "            game.starttime = time.time()\n",
    "\n",
    "        self.lastAltitude = int(vessel['altitude'])\n",
    "\n",
    "        # If no movement is 10 seconds end episode\n",
    "        if (time.time() - game.starttime) > 10:\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        return np.array(state, dtype='f'), reward, done\n",
    "    \n",
    "    def create_environment(self):\n",
    "        game = KSPDeepEngine()\n",
    "\n",
    "        noop = KSPAction()\n",
    "\n",
    "        roll_left = KSPAction()\n",
    "        roll_left.flightCtrlState.roll = -1.0\n",
    "\n",
    "        roll_right = KSPAction()\n",
    "        roll_right.flightCtrlState.roll = 1.0\n",
    "\n",
    "        yaw_left = KSPAction()\n",
    "        yaw_left.flightCtrlState.yaw = -1.0\n",
    "\n",
    "        yaw_right = KSPAction()\n",
    "        yaw_right.flightCtrlState.yaw = 1.0\n",
    "\n",
    "        pitch_left = KSPAction()\n",
    "        pitch_left.flightCtrlState.pitch = -1.0\n",
    "\n",
    "        pitch_right = KSPAction()\n",
    "        pitch_right.flightCtrlState.pitch = 1.0\n",
    "\n",
    "        throttle_0 = KSPAction()\n",
    "        throttle_0.flightCtrlState.mainThrottle = 0.0\n",
    "\n",
    "        throttle_100 = KSPAction()\n",
    "        throttle_100.flightCtrlState.mainThrottle = 1.0\n",
    "\n",
    "        stage = KSPAction()\n",
    "        stage.action = 1\n",
    "\n",
    "        action_map = [\n",
    "            'mainThrottle', 'mainThrottle', 'noop', 'roll', 'roll', 'yaw', 'yaw', 'pitch', 'pitch'\n",
    "        ]\n",
    "\n",
    "        possible_actions = [\n",
    "            throttle_100, throttle_0, noop, roll_left, roll_right, yaw_left, yaw_right,\n",
    "            pitch_left, pitch_right\n",
    "        ]\n",
    "\n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = self.parseState(vessel, state, game)\n",
    "        print('connected to KSP')\n",
    "\n",
    "        return game, possible_actions, action_map, state.shape\n",
    "\n",
    "    def set_action(self, vessel, action, action_map):\n",
    "        flightCtrl = vessel.flightCtrlState\n",
    "        a = KSPAction()\n",
    "\n",
    "        a.flightCtrlState = action.flightCtrlState\n",
    "        if action_map == 'mainThrottle':\n",
    "            a.flightCtrlState.mainThrottle = action.flightCtrlState.mainThrottle\n",
    "        else:\n",
    "            a.flightCtrlState.mainThrottle = flightCtrl.mainThrottle\n",
    "\n",
    "        if action_map == 'stage':\n",
    "            a.action = 1\n",
    "\n",
    "        return a\n",
    "\n",
    "    def select_random_action(self):\n",
    "        index = randrange(len(possible_actions))    \n",
    "        action = possible_actions[index]\n",
    "        action_key = action_map[index]\n",
    "        return action, action_key, index\n",
    "    \n",
    "    def new_episode(self):\n",
    "        game.new_episode()\n",
    "        self.lastCheckAlt = 89\n",
    "        self.lastAltitude = 89\n",
    "        time.sleep(1.5)\n",
    "        action = KSPAction()\n",
    "        action.action = KSPAction.STAGING\n",
    "        self.game.get_state(action)\n",
    "        time.sleep(1.5)\n",
    "        action = KSPAction()\n",
    "        action.action = KSPAction.STAGING        \n",
    "        self.game.get_state(action)\n",
    "        self.game.start = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_environment\n",
    "This will setup the possible actions you want your network to control. Currently the actions are roll, pitch, and yaw left and right, throttle to 0% or 100%, and a noop action.\n",
    "\n",
    "### Input Filter\n",
    "The input filter is a way to select which values will be added to the state array. A full list can be found the docs or in the KSPDeepEngine.py file.\n",
    "\n",
    "\n",
    "```python\n",
    "INPUTFILTER = {}  \n",
    "INPUTFILTER['currentStage'] = 'int'\n",
    "INPUTFILTER['altitude'] = 'float'\n",
    "INPUTFILTER['velocityD'] = 'vector'\n",
    "```\n",
    "\n",
    "### noop\n",
    "The noop action does not effect throttle settings. Throttle is a sticky action so once it is set to it will stay that value until you set it again.  Other control actions always reset to 0. Of course you can modify this if you wish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to KSP\n"
     ]
    }
   ],
   "source": [
    "challenge1 = Challenge1()\n",
    "game, possible_actions, action_map, shape = challenge1.create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Var\n",
    "Here we will set all the common variables that we can adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stack Frame Size\n",
    "stack_size = 1 # How many frames to feed into the network\n",
    "\n",
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [*shape, stack_size]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = len(possible_actions) # 3 possible actions: left, right, shoot\n",
    "action_space = np.identity(action_size)\n",
    "\n",
    "learning_rate =  0.0001     # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 200        # Total episodes for training\n",
    "max_steps = 10000              # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "action_delay = 0.5          #How long to wait to check next_action\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.10            # minimum exploration probability \n",
    "decay_rate = 0.001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.98               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000         # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "train_number = 2 # Change this for each training\n",
    "\n",
    "model_path = './modules/model.ckpt'\n",
    "\n",
    "testing = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment():\n",
    "    episodes = 1\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        index = randrange(len(possible_actions))\n",
    "        action = possible_actions[index]\n",
    "        total_reward = 0\n",
    "    \n",
    "        vessel, state = game.get_state(action)\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "\n",
    "        while not done:\n",
    "            time.sleep(0.2)\n",
    "            index = randrange(len(possible_actions))\n",
    "            action = possible_actions[index]\n",
    "            action_key = action_map[index]\n",
    "            \n",
    "            action = challenge1.set_action(vessel, action, action_key)\n",
    "            vessel, state = game.get_state(action)\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "\n",
    "            total_reward = reward\n",
    "        print (\"Episode: \", str(i), \"Reward:\", total_reward)\n",
    "        time.sleep(2)\n",
    "\n",
    "if testing:\n",
    "    test_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    #Add any preprocessing to frame here\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_frames  =  deque([np.zeros((*shape), dtype=np.int) for i in range(stack_size)], maxlen=stack_size) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((*shape), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for i in range(stack_size):    \n",
    "            stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=1)\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=1) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Network Memory\n",
    "Here we will send random action to the rocket and save the results to memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Pretrain\n",
      "End Pretrain\n"
     ]
    }
   ],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Start New Episode\n",
    "challenge1.new_episode()\n",
    "vessel, state = game.get_state()\n",
    "state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "print('Start Pretrain')\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # Select Random Action\n",
    "    action, action_key, index = challenge1.select_random_action()\n",
    "    action = challenge1.set_action(vessel, action, action_key)\n",
    "         \n",
    "    vessel, state = game.get_state(action)\n",
    "    state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, False)    \n",
    "    \n",
    "    time.sleep(action_delay)\n",
    "        \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros((*shape), dtype=np.int)\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action_space[index], reward, next_state, done))\n",
    "\n",
    "        # Start a new episode\n",
    "        challenge1.new_episode()\n",
    "               \n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        # Get the next state\n",
    "        vessel, next_state = game.get_state()\n",
    "        next_state, reward, done = challenge1.parseState(vessel, next_state, game)\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action_space[index], reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "print('End Pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../agents/BasicDQN.py:9: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../agents/BasicDQN.py:12: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../agents/BasicDQN.py:19: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f8870a79278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f8870a79278>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f8870a79278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f8870a79278>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ../../agents/BasicDQN.py:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8871df6780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From ../../agents/BasicDQN.py:39: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/michael/projects/ksp-deepengine/env/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the BasicDQN\n",
    "BasicDQN = BasicDQN(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand() - 0.1\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    action, _, index = challenge1.select_random_action()\n",
    "\n",
    "    if (explore_probability <= exp_exp_tradeoff):\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        #print(choice)\n",
    "        action = possible_actions[int(choice)]\n",
    "        index = int(choice)        \n",
    "                \n",
    "    return action, explore_probability, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Episode: 0 Max Altitude: 112.5891 Training loss: 1.0403 Explore P: 0.9769 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 1 Max Altitude: 836.9823 Training loss: 0.5363 Explore P: 0.9258 Episode Reward: 11.0000\n",
      "Episode: 2 Max Altitude: 212.5379 Training loss: 0.6961 Explore P: 0.8958 Episode Reward: 1.0000\n",
      "Episode: 3 Max Altitude: 796.2223 Training loss: 0.7793 Explore P: 0.8495 Episode Reward: 11.0000\n",
      "Episode: 4 Max Altitude: 373.7213 Training loss: 0.9906 Explore P: 0.8165 Episode Reward: 6.0000\n",
      "Episode: 5 Max Altitude: 3417.7838 Training loss: 2.7852 Explore P: 0.7317 Episode Reward: 20.0000\n",
      "Model Saved\n",
      "Episode: 6 Max Altitude: 89.4751 Training loss: 1.4646 Explore P: 0.7173 Episode Reward: -2.0000\n",
      "Episode: 7 Max Altitude: 89.4599 Training loss: 2.0635 Explore P: 0.7045 Episode Reward: -2.0000\n",
      "Episode: 8 Max Altitude: 509.9095 Training loss: 4.4853 Explore P: 0.6785 Episode Reward: 7.0000\n",
      "Episode: 9 Max Altitude: 299.8803 Training loss: 0.7453 Explore P: 0.6575 Episode Reward: 4.0000\n",
      "Episode: 10 Max Altitude: 89.4766 Training loss: 1.6199 Explore P: 0.6448 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 11 Max Altitude: 332.3672 Training loss: 3.0188 Explore P: 0.6266 Episode Reward: 4.0000\n",
      "Episode: 12 Max Altitude: 3963.8620 Training loss: 3.7444 Explore P: 0.5633 Episode Reward: 26.0000\n",
      "Episode: 13 Max Altitude: 2026.8959 Training loss: 3.9105 Explore P: 0.5222 Episode Reward: 20.0000\n",
      "Episode: 14 Max Altitude: 7247.0249 Training loss: 7.7131 Explore P: 0.4505 Episode Reward: 30.0000\n",
      "Episode: 15 Max Altitude: 8008.6422 Training loss: 3.4085 Explore P: 0.3904 Episode Reward: 51.0000\n",
      "Model Saved\n",
      "Episode: 16 Max Altitude: 160.0140 Training loss: 0.7615 Explore P: 0.3836 Episode Reward: -1.0000\n",
      "Episode: 17 Max Altitude: 748.0428 Training loss: 3.8455 Explore P: 0.3657 Episode Reward: 17.0000\n",
      "Episode: 18 Max Altitude: 2227.4212 Training loss: 7.9124 Explore P: 0.3411 Episode Reward: 19.0000\n",
      "Episode: 19 Max Altitude: 1816.2153 Training loss: 2.4723 Explore P: 0.3208 Episode Reward: 22.0000\n",
      "Episode: 20 Max Altitude: 89.4599 Training loss: 4.1329 Explore P: 0.3160 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 21 Max Altitude: 1681.3332 Training loss: 5.8874 Explore P: 0.2994 Episode Reward: 19.0000\n",
      "Episode: 22 Max Altitude: 191.6410 Training loss: 5.8908 Explore P: 0.2926 Episode Reward: 1.0000\n",
      "Episode: 23 Max Altitude: 1405.2128 Training loss: 2.4124 Explore P: 0.2788 Episode Reward: 14.0000\n",
      "Episode: 24 Max Altitude: 3769.7341 Training loss: 7.4509 Explore P: 0.2550 Episode Reward: 44.0000\n",
      "Episode: 25 Max Altitude: 465.0646 Training loss: 1.9795 Explore P: 0.2488 Episode Reward: 7.0000\n",
      "Model Saved\n",
      "Episode: 26 Max Altitude: 5531.3913 Training loss: 6.7110 Explore P: 0.2284 Episode Reward: 28.0000\n",
      "Episode: 27 Max Altitude: 768.3737 Training loss: 7.1247 Explore P: 0.2213 Episode Reward: 14.0000\n",
      "Episode: 28 Max Altitude: 89.5305 Training loss: 14.3553 Explore P: 0.2192 Episode Reward: -2.0000\n",
      "Episode: 29 Max Altitude: 605.3336 Training loss: 9.5483 Explore P: 0.2139 Episode Reward: 9.0000\n",
      "Episode: 30 Max Altitude: 4093.9248 Training loss: 1.4550 Explore P: 0.2003 Episode Reward: 28.0000\n",
      "Model Saved\n",
      "Episode: 31 Max Altitude: 37920.3334 Training loss: 17.2985 Explore P: 0.1644 Episode Reward: 57.0000\n",
      "Episode: 32 Max Altitude: 3474.0125 Training loss: 10.2894 Explore P: 0.1566 Episode Reward: 22.0000\n",
      "Episode: 33 Max Altitude: 16772.8829 Training loss: 6.0740 Explore P: 0.1443 Episode Reward: 78.0000\n",
      "Episode: 34 Max Altitude: 6978.9047 Training loss: 7.1951 Explore P: 0.1371 Episode Reward: 31.0000\n",
      "Episode: 35 Max Altitude: 2299.4311 Training loss: 9.5710 Explore P: 0.1335 Episode Reward: 20.0000\n",
      "Model Saved\n",
      "Episode: 36 Max Altitude: 444.7241 Training loss: 18.1778 Explore P: 0.1321 Episode Reward: 10.0000\n",
      "Episode: 37 Max Altitude: 110.5863 Training loss: 9.9826 Explore P: 0.1310 Episode Reward: -2.0000\n",
      "Episode: 38 Max Altitude: 2001.5412 Training loss: 8.9666 Explore P: 0.1282 Episode Reward: 21.0000\n",
      "Episode: 39 Max Altitude: 2319.9264 Training loss: 7.8770 Explore P: 0.1251 Episode Reward: 26.0000\n",
      "Episode: 40 Max Altitude: 108.3533 Training loss: 14.2215 Explore P: 0.1244 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 41 Max Altitude: 56179.1450 Training loss: 15.7242 Explore P: 0.1147 Episode Reward: 79.0000\n",
      "Episode: 42 Max Altitude: 78105.0451 Training loss: 14.8218 Explore P: 0.1083 Episode Reward: 57.0000\n",
      "Episode: 43 Max Altitude: 1082.8692 Training loss: 3.7754 Explore P: 0.1077 Episode Reward: 18.0000\n",
      "Episode: 44 Max Altitude: 11358.7819 Training loss: 1.7345 Explore P: 0.1062 Episode Reward: 64.0000\n",
      "Episode: 45 Max Altitude: 5387.3810 Training loss: 1.8964 Explore P: 0.1053 Episode Reward: 22.0000\n",
      "Model Saved\n",
      "Episode: 46 Max Altitude: 19465.5218 Training loss: 0.8586 Explore P: 0.1038 Episode Reward: 36.0000\n",
      "Episode: 47 Max Altitude: 424.4894 Training loss: 1.2972 Explore P: 0.1036 Episode Reward: 4.0000\n",
      "Episode: 48 Max Altitude: 87989.0491 Training loss: 12.0441 Explore P: 0.1019 Episode Reward: 121.0000\n",
      "Episode: 49 Max Altitude: 89.4575 Training loss: 22.9608 Explore P: 0.1019 Episode Reward: -3.0000\n",
      "Episode: 50 Max Altitude: 1146.9013 Training loss: 8.0237 Explore P: 0.1017 Episode Reward: 16.0000\n",
      "Model Saved\n",
      "Episode: 51 Max Altitude: 137.2588 Training loss: 14.6352 Explore P: 0.1017 Episode Reward: -2.0000\n",
      "Episode: 52 Max Altitude: 4146.1888 Training loss: 2.2410 Explore P: 0.1014 Episode Reward: 20.0000\n",
      "Episode: 53 Max Altitude: 1066.1230 Training loss: 1.0123 Explore P: 0.1013 Episode Reward: 14.0000\n",
      "Episode: 54 Max Altitude: 89.4575 Training loss: 37.4190 Explore P: 0.1013 Episode Reward: -3.0000\n",
      "Episode: 55 Max Altitude: 97.4641 Training loss: 15.7076 Explore P: 0.1013 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 56 Max Altitude: 4000.5075 Training loss: 20.7197 Explore P: 0.1011 Episode Reward: 26.0000\n",
      "Episode: 57 Max Altitude: 1034.2567 Training loss: 3.5560 Explore P: 0.1010 Episode Reward: 11.0000\n",
      "Episode: 58 Max Altitude: 89.4575 Training loss: 14.0105 Explore P: 0.1010 Episode Reward: -1.0000\n",
      "Episode: 59 Max Altitude: 89.4599 Training loss: 23.1819 Explore P: 0.1010 Episode Reward: -2.0000\n",
      "Episode: 60 Max Altitude: 89.4575 Training loss: 11.5606 Explore P: 0.1010 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 61 Max Altitude: 89.4575 Training loss: 10.7193 Explore P: 0.1009 Episode Reward: -2.0000\n",
      "Episode: 62 Max Altitude: 2266.8647 Training loss: 12.6387 Explore P: 0.1009 Episode Reward: 17.0000\n",
      "Episode: 63 Max Altitude: 89.4575 Training loss: 29.2679 Explore P: 0.1008 Episode Reward: -2.0000\n",
      "Episode: 64 Max Altitude: 89.4599 Training loss: 26.5335 Explore P: 0.1008 Episode Reward: -3.0000\n",
      "Episode: 65 Max Altitude: 199.6163 Training loss: 42.2450 Explore P: 0.1008 Episode Reward: 0.0000\n",
      "Model Saved\n",
      "Episode: 66 Max Altitude: 89.4575 Training loss: 36.7447 Explore P: 0.1008 Episode Reward: -3.0000\n",
      "Episode: 67 Max Altitude: 89.4575 Training loss: 28.8850 Explore P: 0.1007 Episode Reward: -1.0000\n",
      "Episode: 68 Max Altitude: 89.4624 Training loss: 22.6250 Explore P: 0.1007 Episode Reward: -3.0000\n",
      "Episode: 69 Max Altitude: 89.4575 Training loss: 1.6764 Explore P: 0.1007 Episode Reward: -3.0000\n",
      "Episode: 70 Max Altitude: 89.4575 Training loss: 1.0475 Explore P: 0.1007 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 71 Max Altitude: 89.5165 Training loss: 19.0051 Explore P: 0.1007 Episode Reward: -2.0000\n",
      "Episode: 72 Max Altitude: 89.4575 Training loss: 25.2117 Explore P: 0.1007 Episode Reward: -3.0000\n",
      "Episode: 73 Max Altitude: 89.4575 Training loss: 20.1671 Explore P: 0.1006 Episode Reward: -3.0000\n",
      "Episode: 74 Max Altitude: 3918.3660 Training loss: 30.0698 Explore P: 0.1006 Episode Reward: 26.0000\n",
      "Episode: 75 Max Altitude: 89.4656 Training loss: 25.5232 Explore P: 0.1006 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 76 Max Altitude: 89.4575 Training loss: 12.5995 Explore P: 0.1005 Episode Reward: -2.0000\n",
      "Episode: 77 Max Altitude: 89.4575 Training loss: 0.6271 Explore P: 0.1005 Episode Reward: -2.0000\n",
      "Episode: 78 Max Altitude: 89.5120 Training loss: 4.8012 Explore P: 0.1005 Episode Reward: -2.0000\n",
      "Episode: 79 Max Altitude: 89.4575 Training loss: 10.8202 Explore P: 0.1005 Episode Reward: -3.0000\n",
      "Episode: 80 Max Altitude: 89.4575 Training loss: 21.5117 Explore P: 0.1005 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 81 Max Altitude: 89.4584 Training loss: 0.9764 Explore P: 0.1005 Episode Reward: -3.0000\n",
      "Episode: 82 Max Altitude: 89.4575 Training loss: 41.4301 Explore P: 0.1005 Episode Reward: -3.0000\n",
      "Episode: 83 Max Altitude: 89.4575 Training loss: 4.7092 Explore P: 0.1005 Episode Reward: -2.0000\n",
      "Episode: 84 Max Altitude: 89.5736 Training loss: 11.4285 Explore P: 0.1005 Episode Reward: -1.0000\n",
      "Episode: 85 Max Altitude: 89.4575 Training loss: 13.4395 Explore P: 0.1004 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 86 Max Altitude: 89.4575 Training loss: 19.1470 Explore P: 0.1004 Episode Reward: -2.0000\n",
      "Episode: 87 Max Altitude: 89.4621 Training loss: 13.2327 Explore P: 0.1004 Episode Reward: -3.0000\n",
      "Episode: 88 Max Altitude: 89.4575 Training loss: 5.1382 Explore P: 0.1004 Episode Reward: -3.0000\n",
      "Episode: 89 Max Altitude: 89.5323 Training loss: 2.0192 Explore P: 0.1004 Episode Reward: -2.0000\n",
      "Episode: 90 Max Altitude: 89.4575 Training loss: 26.0103 Explore P: 0.1004 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 91 Max Altitude: 614.5168 Training loss: 25.3784 Explore P: 0.1004 Episode Reward: 5.0000\n",
      "Episode: 92 Max Altitude: 89.4575 Training loss: 18.4856 Explore P: 0.1004 Episode Reward: -3.0000\n",
      "Episode: 93 Max Altitude: 89.5323 Training loss: 15.0660 Explore P: 0.1004 Episode Reward: -2.0000\n",
      "Episode: 94 Max Altitude: 2831.2288 Training loss: 13.4760 Explore P: 0.1003 Episode Reward: 22.0000\n",
      "Episode: 95 Max Altitude: 89.4575 Training loss: 1.2413 Explore P: 0.1003 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 96 Max Altitude: 89.5114 Training loss: 9.5738 Explore P: 0.1003 Episode Reward: -3.0000\n",
      "Episode: 97 Max Altitude: 123.6211 Training loss: 1.0632 Explore P: 0.1003 Episode Reward: -2.0000\n",
      "Episode: 98 Max Altitude: 5936.2103 Training loss: 10.5352 Explore P: 0.1002 Episode Reward: 19.0000\n",
      "Episode: 99 Max Altitude: 89.4575 Training loss: 17.9845 Explore P: 0.1002 Episode Reward: -3.0000\n",
      "Episode: 100 Max Altitude: 89.4575 Training loss: 11.0446 Explore P: 0.1002 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 101 Max Altitude: 6810.5474 Training loss: 26.1481 Explore P: 0.1002 Episode Reward: 32.0000\n",
      "Episode: 102 Max Altitude: 89.4575 Training loss: 4.1959 Explore P: 0.1002 Episode Reward: -3.0000\n",
      "Episode: 103 Max Altitude: 365.9738 Training loss: 11.9428 Explore P: 0.1002 Episode Reward: 5.0000\n",
      "Episode: 104 Max Altitude: 4001.9008 Training loss: 12.9776 Explore P: 0.1001 Episode Reward: 26.0000\n",
      "Episode: 105 Max Altitude: 113.9861 Training loss: 10.9761 Explore P: 0.1001 Episode Reward: -4.0000\n",
      "Model Saved\n",
      "Episode: 106 Max Altitude: 89.4575 Training loss: 1.5273 Explore P: 0.1001 Episode Reward: -2.0000\n",
      "Episode: 107 Max Altitude: 4112.7672 Training loss: 7.0456 Explore P: 0.1001 Episode Reward: 22.0000\n",
      "Episode: 108 Max Altitude: 89.4575 Training loss: 2.9356 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 109 Max Altitude: 89.4575 Training loss: 9.2188 Explore P: 0.1001 Episode Reward: -2.0000\n",
      "Episode: 110 Max Altitude: 1199.4119 Training loss: 11.8425 Explore P: 0.1001 Episode Reward: 17.0000\n",
      "Model Saved\n",
      "Episode: 111 Max Altitude: 89.4575 Training loss: 6.3750 Explore P: 0.1001 Episode Reward: -1.0000\n",
      "Episode: 112 Max Altitude: 89.4575 Training loss: 9.9615 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 113 Max Altitude: 894.4701 Training loss: 2.8516 Explore P: 0.1001 Episode Reward: 16.0000\n",
      "Episode: 114 Max Altitude: 89.4575 Training loss: 5.1156 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 115 Max Altitude: 6343.6667 Training loss: 10.7741 Explore P: 0.1001 Episode Reward: 48.0000\n",
      "Model Saved\n",
      "Episode: 116 Max Altitude: 89.4575 Training loss: 0.7998 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 117 Max Altitude: 928.2338 Training loss: 18.6695 Explore P: 0.1001 Episode Reward: 17.0000\n",
      "Episode: 118 Max Altitude: 89.4575 Training loss: 17.4476 Explore P: 0.1001 Episode Reward: -2.0000\n",
      "Episode: 119 Max Altitude: 89.4575 Training loss: 12.4481 Explore P: 0.1001 Episode Reward: -2.0000\n",
      "Episode: 120 Max Altitude: 89.4575 Training loss: 6.5487 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 121 Max Altitude: 89.4575 Training loss: 3.0410 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 122 Max Altitude: 89.4575 Training loss: 19.7758 Explore P: 0.1001 Episode Reward: -2.0000\n",
      "Episode: 123 Max Altitude: 89.4575 Training loss: 12.3382 Explore P: 0.1001 Episode Reward: -3.0000\n",
      "Episode: 124 Max Altitude: 672.3578 Training loss: 19.3177 Explore P: 0.1001 Episode Reward: 16.0000\n",
      "Episode: 125 Max Altitude: 89.4575 Training loss: 11.6477 Explore P: 0.1001 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 126 Max Altitude: 89.4574 Training loss: 16.2323 Explore P: 0.1001 Episode Reward: -1.0000\n",
      "Episode: 127 Max Altitude: 89.4575 Training loss: 25.3643 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 128 Max Altitude: 89.4575 Training loss: 14.0036 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 129 Max Altitude: 250.6554 Training loss: 3.3380 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 130 Max Altitude: 89.4575 Training loss: 28.8190 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Model Saved\n",
      "Episode: 131 Max Altitude: 89.4599 Training loss: 12.3264 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Episode: 132 Max Altitude: 89.4575 Training loss: 28.6306 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 133 Max Altitude: 3283.9153 Training loss: 10.8927 Explore P: 0.1000 Episode Reward: 31.0000\n",
      "Episode: 134 Max Altitude: 3298.0596 Training loss: 4.2053 Explore P: 0.1000 Episode Reward: 33.0000\n",
      "Episode: 135 Max Altitude: 89.4575 Training loss: 10.6850 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 136 Max Altitude: 89.4574 Training loss: 28.9100 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 137 Max Altitude: 89.4575 Training loss: 23.2315 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 138 Max Altitude: 89.5324 Training loss: 11.6982 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 139 Max Altitude: 274.7059 Training loss: 18.7048 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 140 Max Altitude: 89.4574 Training loss: 11.5026 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 141 Max Altitude: 89.4575 Training loss: 0.7214 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 142 Max Altitude: 89.4575 Training loss: 9.8952 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 143 Max Altitude: 89.4574 Training loss: 2.8176 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 144 Max Altitude: 89.4574 Training loss: 2.4483 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Episode: 145 Max Altitude: 89.4770 Training loss: 1.9241 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 146 Max Altitude: 89.4575 Training loss: 3.6094 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 147 Max Altitude: 89.4570 Training loss: 10.4100 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 148 Max Altitude: 89.4577 Training loss: 21.1003 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 149 Max Altitude: 89.4562 Training loss: 7.2230 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 150 Max Altitude: 89.4582 Training loss: 21.3260 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 151 Max Altitude: 7223.9234 Training loss: 32.8752 Explore P: 0.1000 Episode Reward: 41.0000\n",
      "Episode: 152 Max Altitude: 9219.5346 Training loss: 17.9056 Explore P: 0.1000 Episode Reward: 78.0000\n",
      "Episode: 153 Max Altitude: 11066.8649 Training loss: 1.1444 Explore P: 0.1000 Episode Reward: 43.0000\n",
      "Episode: 154 Max Altitude: 89.4580 Training loss: 2.5424 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 155 Max Altitude: 89.4593 Training loss: 17.7278 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 156 Max Altitude: 89.4568 Training loss: 7.6835 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 157 Max Altitude: 89.4891 Training loss: 1.0475 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Episode: 158 Max Altitude: 1099.1889 Training loss: 1.1904 Explore P: 0.1000 Episode Reward: 30.0000\n",
      "Episode: 159 Max Altitude: 89.4592 Training loss: 20.1204 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Episode: 160 Max Altitude: 89.4552 Training loss: 6.5347 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 161 Max Altitude: 89.4575 Training loss: 2.3633 Explore P: 0.1000 Episode Reward: -4.0000\n",
      "Episode: 162 Max Altitude: 89.4575 Training loss: 5.6160 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 163 Max Altitude: 89.4540 Training loss: 1.7079 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 164 Max Altitude: 89.4640 Training loss: 28.0875 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 165 Max Altitude: 89.4678 Training loss: 0.9346 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 166 Max Altitude: 89.4540 Training loss: 15.2279 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 167 Max Altitude: 89.5694 Training loss: 8.7123 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 168 Max Altitude: 89.4583 Training loss: 4.6039 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 169 Max Altitude: 89.5096 Training loss: 4.7309 Explore P: 0.1000 Episode Reward: -3.0000\n",
      "Episode: 170 Max Altitude: 118.0202 Training loss: 7.1996 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 171 Max Altitude: 9612.7417 Training loss: 3.4262 Explore P: 0.1000 Episode Reward: 59.0000\n",
      "Episode: 172 Max Altitude: 601.1877 Training loss: 3.0104 Explore P: 0.1000 Episode Reward: 11.0000\n",
      "Episode: 173 Max Altitude: 85.4825 Training loss: 14.0785 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 174 Max Altitude: 85.3976 Training loss: 8.3635 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 175 Max Altitude: 89.5694 Training loss: 27.8067 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 176 Max Altitude: 85.4168 Training loss: 22.9330 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 177 Max Altitude: 89.5694 Training loss: 1.0752 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 178 Max Altitude: 89.5694 Training loss: 12.3435 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 179 Max Altitude: 89.5694 Training loss: 26.8245 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 180 Max Altitude: 89.5694 Training loss: 5.6954 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 181 Max Altitude: 89.5694 Training loss: 1.5157 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 182 Max Altitude: 89.5694 Training loss: 13.5053 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 183 Max Altitude: 89.5694 Training loss: 5.6880 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 184 Max Altitude: 89.5694 Training loss: 8.7372 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 185 Max Altitude: 89.5694 Training loss: 5.0538 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Model Saved\n",
      "Episode: 186 Max Altitude: 89.5694 Training loss: 16.8046 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 187 Max Altitude: 89.5694 Training loss: 14.4197 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 188 Max Altitude: 89.5694 Training loss: 5.1187 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 189 Max Altitude: 89.5694 Training loss: 3.4105 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Episode: 190 Max Altitude: 89.5694 Training loss: 2.2347 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 191 Max Altitude: 89.5694 Training loss: 8.6685 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 192 Max Altitude: 89.5694 Training loss: 8.4078 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 193 Max Altitude: 590.5661 Training loss: 1.1530 Explore P: 0.1000 Episode Reward: 8.0000\n",
      "Episode: 194 Max Altitude: 159.6176 Training loss: 12.2032 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 195 Max Altitude: 127.7183 Training loss: 14.2281 Explore P: 0.1000 Episode Reward: -1.0000\n",
      "Model Saved\n",
      "Episode: 196 Max Altitude: 85.5729 Training loss: 14.3241 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 197 Max Altitude: 85.4517 Training loss: 10.6355 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 198 Max Altitude: 85.4661 Training loss: 16.6764 Explore P: 0.1000 Episode Reward: -2.0000\n",
      "Episode: 199 Max Altitude: 152.0311 Training loss: 16.9522 Explore P: 0.1000 Episode Reward: -1.0000\n"
     ]
    }
   ],
   "source": [
    "if training:\n",
    "    print('Start Training')\n",
    "    # Setup TensorBoard Writer\n",
    "    writer = tf.summary.FileWriter(\"tensorboard/challenge1/\" + str(train_number))\n",
    "\n",
    "    tf.summary.scalar(\"Loss\", BasicDQN.loss)\n",
    "    write_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            episode_max_height = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            challenge1.new_episode()\n",
    "            vessel, state = game.get_state()\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            while not done:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability, index = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                # Do the action\n",
    "                action = challenge1.set_action(vessel, action, action_map[index])\n",
    "                vessel, state = game.get_state(action)\n",
    "                state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "                state, stacked_frames = stack_frames(stacked_frames, state, False)\n",
    "\n",
    "                # Add the reward to total reward\n",
    "                if vessel.vessel['altitude'] > episode_max_height:\n",
    "                    episode_max_height = vessel.vessel['altitude']\n",
    "                time.sleep(action_delay)\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    episode_rewards.append(reward)\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((*shape), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Max Altitude: {:.4f}'.format(episode_max_height),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability),\n",
    "                              'Episode Reward: {:.4f}'.format(total_reward))\n",
    "\n",
    "                    memory.add((state, action_space[index], reward, next_state, done))\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    vessel, next_state = game.get_state()\n",
    "                    next_state, reward, _ = challenge1.parseState(vessel, next_state, game)\n",
    "                    episode_rewards.append(reward)\n",
    "\n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((preprocess_frame(state), action_space[index], reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "            \n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "\n",
    "                loss, _ = sess.run([BasicDQN.loss, BasicDQN.optimizer],\n",
    "                                    feed_dict={BasicDQN.inputs_: states_mb,\n",
    "                                               BasicDQN.target_Q: targets_mb,\n",
    "                                               BasicDQN.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={BasicDQN.inputs_: states_mb,\n",
    "                                                   BasicDQN.target_Q: targets_mb,\n",
    "                                                   BasicDQN.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, model_path)\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and test model\n",
      "INFO:tensorflow:Restoring parameters from ./modules/model.ckpt\n",
      "Score:  132\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Load and test model')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #game, possible_actions = create_environment()    \n",
    "    # Load the model\n",
    "    saver.restore(sess, model_path)\n",
    "    for i in range(1):\n",
    "        episode_rewards = []\n",
    "\n",
    "        done = False\n",
    "\n",
    "        challenge1.new_episode()\n",
    "        \n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while not done:\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: state.reshape((1, *state.shape))})\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            vessel, state = game.get_state(action)\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        \n",
    "            time.sleep(action_delay)\n",
    "            if done:\n",
    "                break  \n",
    "            else:\n",
    "                vessel, next_state = game.get_state()\n",
    "                next_state, reward, done = challenge1.parseState(vessel, next_state, game)\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                episode_rewards.append(reward)\n",
    "                state = next_state\n",
    "\n",
    "        score = np.sum(episode_rewards)\n",
    "        print(\"Score: \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
