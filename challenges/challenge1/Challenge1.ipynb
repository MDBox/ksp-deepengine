{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "Fly as high as you can!\n",
    "\n",
    "## Objective\n",
    "Build a network that can launch a satallite rocket and fly it to the highest altitude possible. You can use the notebook below to get started. See if you can modify the training to include staging the rocket so it will go even higher.\n",
    "\n",
    "## Rules\n",
    "* You must use the provided ship\n",
    "* Don't escape Kerbin orbit\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libs\n",
    "import tensorflow as tf      \n",
    "import numpy as np\n",
    "import random                \n",
    "from random import randrange\n",
    "import time                  \n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import KSP Client Engine\n",
    "import sys\n",
    "from os.path import dirname\n",
    "sys.path.append(dirname('../../lib'))\n",
    "sys.path.append(dirname('../../agents'))\n",
    "from lib.DeepEngine.KSPDeepEngine import *\n",
    "from agents.BasicDQN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These function will help us parse and process the game state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTFILTER = {}\n",
    "INPUTFILTER['currentStage'] = 'int'\n",
    "INPUTFILTER['velocityD'] = 'vector'\n",
    "INPUTFILTER['upAxis'] = 'vector'\n",
    "INPUTFILTER['up'] = 'vector'\n",
    "INPUTFILTER['north'] = 'vector'\n",
    "INPUTFILTER['east'] = 'vector'\n",
    "\n",
    "class Challenge1:    \n",
    "    def __init__(self):\n",
    "        # These inital values are set to 89M because that is the start alt of the episode\n",
    "        self.lastCheckAlt = 89\n",
    "        self.lastAltitude = 89\n",
    "        self.game = KSPDeepEngine()\n",
    "        \n",
    "    def parseVector3(self, vector3):\n",
    "        mag = math.sqrt(vector3['x'] * vector3['x'] + vector3['y'] * vector3['y'] + vector3['z'] * vector3['z'])\n",
    "        x = 0 \n",
    "        y = 0\n",
    "        z = 0\n",
    "\n",
    "        if mag != 0:\n",
    "            x = vector3['x']/mag\n",
    "            y = vector3['y']/mag\n",
    "            z = vector3['z']/mag\n",
    "        return [x,y,z]\n",
    "\n",
    "    def parseVessel(self, vessel):\n",
    "        values = []\n",
    "        for k in vessel:\n",
    "            if k in INPUTFILTER:\n",
    "                if INPUTFILTER[k] == 'int':\n",
    "                    values.append(vessel[k])\n",
    "                if INPUTFILTER[k] == 'float':\n",
    "                    values.append(math.atan(vessel[k]))\n",
    "                if INPUTFILTER[k] == 'vector':\n",
    "                    values.extend(self.parseVector3(vessel[k]))            \n",
    "        return values\n",
    "\n",
    "    def parseState(self, action, state, game):\n",
    "        done = False\n",
    "        vessel = action.vessel\n",
    "        state.extend(self.parseVessel(vessel))    \n",
    "        reward = 0\n",
    "        \n",
    "        if vessel['altitude'] - self.lastCheckAlt > 10:\n",
    "            reward = 1\n",
    "            self.lastCheckAlt = vessel['altitude']\n",
    "        elif self.lastCheckAlt - vessel['altitude'] > 10:\n",
    "            reward = -1\n",
    "            self.lastCheckAlt = vessel['altitude']\n",
    "\n",
    "        # Episode is over if action is = 3 or vessel drops below 75 M\n",
    "        if action.action == 3 or int(vessel['altitude']) <= 75: # Action 3 means ship crashed\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        if int(vessel['altitude']) != self.lastAltitude:\n",
    "            game.starttime = time.time()\n",
    "\n",
    "        self.lastAltitude = int(vessel['altitude'])\n",
    "\n",
    "        # If no movement is 10 seconds end episode\n",
    "        if (time.time() - game.starttime) > 10:\n",
    "            reward = -1\n",
    "            done = True\n",
    "\n",
    "        return np.array(state, dtype='f'), reward, done\n",
    "    \n",
    "    def create_environment(self):\n",
    "        game = KSPDeepEngine()\n",
    "\n",
    "        noop = KSPAction()\n",
    "\n",
    "        roll_left = KSPAction()\n",
    "        roll_left.flightCtrlState.roll = -1.0\n",
    "\n",
    "        roll_right = KSPAction()\n",
    "        roll_right.flightCtrlState.roll = 1.0\n",
    "\n",
    "        yaw_left = KSPAction()\n",
    "        yaw_left.flightCtrlState.yaw = -1.0\n",
    "\n",
    "        yaw_right = KSPAction()\n",
    "        yaw_right.flightCtrlState.yaw = 1.0\n",
    "\n",
    "        pitch_left = KSPAction()\n",
    "        pitch_left.flightCtrlState.pitch = -1.0\n",
    "\n",
    "        pitch_right = KSPAction()\n",
    "        pitch_right.flightCtrlState.pitch = 1.0\n",
    "\n",
    "        throttle_0 = KSPAction()\n",
    "        throttle_0.flightCtrlState.mainThrottle = 0.0\n",
    "\n",
    "        throttle_100 = KSPAction()\n",
    "        throttle_100.flightCtrlState.mainThrottle = 1.0\n",
    "\n",
    "        stage = KSPAction()\n",
    "        stage.action = 1\n",
    "\n",
    "        action_map = [\n",
    "            'mainThrottle', 'mainThrottle', 'noop', 'roll', 'roll', 'yaw', 'yaw', 'pitch', 'pitch'\n",
    "        ]\n",
    "\n",
    "        possible_actions = [\n",
    "            throttle_100, throttle_0, noop, roll_left, roll_right, yaw_left, yaw_right,\n",
    "            pitch_left, pitch_right\n",
    "        ]\n",
    "\n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = self.parseState(vessel, state, game)\n",
    "        print('connected to KSP')\n",
    "\n",
    "        return game, possible_actions, action_map, state.shape\n",
    "\n",
    "    def set_action(self, vessel, action, action_map):\n",
    "        flightCtrl = vessel.flightCtrlState\n",
    "        a = KSPAction()\n",
    "\n",
    "        a.flightCtrlState = action.flightCtrlState\n",
    "        if action_map == 'mainThrottle':\n",
    "            a.flightCtrlState.mainThrottle = action.flightCtrlState.mainThrottle\n",
    "        else:\n",
    "            a.flightCtrlState.mainThrottle = flightCtrl.mainThrottle\n",
    "\n",
    "        if action_map == 'stage':\n",
    "            a.action = 1\n",
    "\n",
    "        return a\n",
    "\n",
    "    def select_random_action(self):\n",
    "        index = randrange(len(possible_actions))    \n",
    "        action = possible_actions[index]\n",
    "        action_key = action_map[index]\n",
    "        return action, action_key, index\n",
    "    \n",
    "    def new_episode(self):\n",
    "        game.new_episode()\n",
    "        self.lastCheckAlt = 89\n",
    "        self.lastAltitude = 89\n",
    "        time.sleep(1.5)\n",
    "        action = KSPAction()\n",
    "        action.action = KSPAction.STAGING\n",
    "        self.game.get_state(action)\n",
    "        time.sleep(1.5)\n",
    "        action = KSPAction()\n",
    "        action.action = KSPAction.STAGING        \n",
    "        self.game.get_state(action)\n",
    "        self.game.start = time.time()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_environment\n",
    "This will setup the possible actions you want your network to control. Currently the actions are roll, pitch, and yaw left and right, throttle to 0% or 100%, and a noop action.\n",
    "\n",
    "### Input Filter\n",
    "The input filter is a way to select which values will be added to the state array. A full list can be found the docs or in the KSPDeepEngine.py file.\n",
    "\n",
    "\n",
    "```python\n",
    "INPUTFILTER = {}  \n",
    "INPUTFILTER['currentStage'] = 'int'\n",
    "INPUTFILTER['altitude'] = 'float'\n",
    "INPUTFILTER['velocityD'] = 'vector'\n",
    "```\n",
    "\n",
    "### noop\n",
    "The noop action does not effect throttle settings. Throttle is a sticky action so once it is set to it will stay that value until you set it again.  Other control actions always reset to 0. Of course you can modify this if you wish.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1 = Challenge1()\n",
    "game, possible_actions, action_map, shape = challenge1.create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Var\n",
    "Here we will set all the common variables that we can adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stack Frame Size\n",
    "stack_size = 1 # How many frames to feed into the network\n",
    "\n",
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [*shape, stack_size]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = len(possible_actions) # 3 possible actions: left, right, shoot\n",
    "action_space = np.identity(action_size)\n",
    "\n",
    "learning_rate =  0.0001     # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 200        # Total episodes for training\n",
    "max_steps = 10000              # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "action_delay = 0.5          #How long to wait to check next_action\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.10            # minimum exploration probability \n",
    "decay_rate = 0.001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.98               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000         # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "train_number = 1 # Change this for each training\n",
    "\n",
    "model_path = './modules/model.ckpt'\n",
    "\n",
    "testing = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_environment():\n",
    "    episodes = 1\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        index = randrange(len(possible_actions))\n",
    "        action = possible_actions[index]\n",
    "        total_reward = 0\n",
    "    \n",
    "        vessel, state = game.get_state(action)\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "\n",
    "        while not done:\n",
    "            time.sleep(0.2)\n",
    "            index = randrange(len(possible_actions))\n",
    "            action = possible_actions[index]\n",
    "            action_key = action_map[index]\n",
    "            \n",
    "            action = challenge1.set_action(vessel, action, action_key)\n",
    "            vessel, state = game.get_state(action)\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "\n",
    "            total_reward = reward\n",
    "        print (\"Episode: \", str(i), \"Reward:\", total_reward)\n",
    "        time.sleep(2)\n",
    "\n",
    "if testing:\n",
    "    test_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    #Add any preprocessing to frame here\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_frames  =  deque([np.zeros((*shape), dtype=np.int) for i in range(stack_size)], maxlen=stack_size) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((*shape), dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for i in range(stack_size):    \n",
    "            stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=1)\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=1) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Network Memory\n",
    "Here we will send random action to the rocket and save the results to memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Start New Episode\n",
    "challenge1.new_episode()\n",
    "vessel, state = game.get_state()\n",
    "state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "print('Start Pretrain')\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # Select Random Action\n",
    "    action, action_key, index = challenge1.select_random_action()\n",
    "    action = challenge1.set_action(vessel, action, action_key)\n",
    "         \n",
    "    vessel, state = game.get_state(action)\n",
    "    state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, False)    \n",
    "    \n",
    "    time.sleep(action_delay)\n",
    "        \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros((*shape), dtype=np.int)\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action_space[index], reward, next_state, done))\n",
    "\n",
    "        # Start a new episode\n",
    "        challenge1.new_episode()\n",
    "               \n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        # Get the next state\n",
    "        vessel, next_state = game.get_state()\n",
    "        next_state, reward, done = challenge1.parseState(vessel, next_state, game)\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action_space[index], reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state\n",
    "print('End Pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the BasicDQN\n",
    "BasicDQN = BasicDQN(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With Ïµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand() - 0.1\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    action, _, index = challenge1.select_random_action()\n",
    "\n",
    "    if (explore_probability <= exp_exp_tradeoff):\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        #print(choice)\n",
    "        action = possible_actions[int(choice)]\n",
    "        index = int(choice)        \n",
    "                \n",
    "    return action, explore_probability, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    print('Start Training')\n",
    "    # Setup TensorBoard Writer\n",
    "    writer = tf.summary.FileWriter(\"tensorboard/challenge1/\" + str(train_number))\n",
    "\n",
    "    tf.summary.scalar(\"Loss\", BasicDQN.loss)\n",
    "    write_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            episode_max_height = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            challenge1.new_episode()\n",
    "            vessel, state = game.get_state()\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            while not done:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability, index = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                \n",
    "                # Do the action\n",
    "                action = challenge1.set_action(vessel, action, action_map[index])\n",
    "                vessel, state = game.get_state(action)\n",
    "                state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "                state, stacked_frames = stack_frames(stacked_frames, state, False)\n",
    "\n",
    "                # Add the reward to total reward\n",
    "                if vessel.vessel['altitude'] > episode_max_height:\n",
    "                    episode_max_height = vessel.vessel['altitude']\n",
    "                time.sleep(action_delay)\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    episode_rewards.append(reward)\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((*shape), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Max Altitude: {:.4f}'.format(episode_max_height),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability),\n",
    "                              'Episode Reward: {:.4f}'.format(total_reward))\n",
    "\n",
    "                    memory.add((state, action_space[index], reward, next_state, done))\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    vessel, next_state = game.get_state()\n",
    "                    next_state, reward, _ = challenge1.parseState(vessel, next_state, game)\n",
    "                    episode_rewards.append(reward)\n",
    "\n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((preprocess_frame(state), action_space[index], reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "            \n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "\n",
    "                loss, _ = sess.run([BasicDQN.loss, BasicDQN.optimizer],\n",
    "                                    feed_dict={BasicDQN.inputs_: states_mb,\n",
    "                                               BasicDQN.target_Q: targets_mb,\n",
    "                                               BasicDQN.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={BasicDQN.inputs_: states_mb,\n",
    "                                                   BasicDQN.target_Q: targets_mb,\n",
    "                                                   BasicDQN.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, model_path)\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Load and test model')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    #game, possible_actions = create_environment()    \n",
    "    # Load the model\n",
    "    saver.restore(sess, model_path)\n",
    "    for i in range(1):\n",
    "        episode_rewards = []\n",
    "\n",
    "        done = False\n",
    "\n",
    "        challenge1.new_episode()\n",
    "        \n",
    "        vessel, state = game.get_state()\n",
    "        state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while not done:\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(BasicDQN.output, feed_dict = {BasicDQN.inputs_: state.reshape((1, *state.shape))})\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            vessel, state = game.get_state(action)\n",
    "            state, reward, done = challenge1.parseState(vessel, state, game)\n",
    "        \n",
    "            time.sleep(action_delay)\n",
    "            if done:\n",
    "                break  \n",
    "            else:\n",
    "                vessel, next_state = game.get_state()\n",
    "                next_state, reward, done = challenge1.parseState(vessel, next_state, game)\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                episode_rewards.append(reward)\n",
    "                state = next_state\n",
    "\n",
    "        score = np.sum(episode_rewards)\n",
    "        print(\"Score: \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
